{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [6] Evaluating Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "metrics = {\"overall\": {}, \"class\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test\n",
    "preprocessor = predictor.get_preprocessor()\n",
    "preprocessed_ds = preprocessor.transform(test_ds)\n",
    "values = preprocessed_ds.select_columns(cols=[\"targets\"]).take_all()\n",
    "y_test = np.stack([item[\"targets\"] for item in values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred\n",
    "test_df = test_ds.to_pandas()\n",
    "z = predictor.predict(data=test_df)[\"predictions\"]  # adds text column (in-memory)\n",
    "y_pred = np.stack(z).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prob\n",
    "y_prob = torch.tensor(np.stack(z)).softmax(dim=1).numpy()\n",
    "print (np.shape(y_test))\n",
    "print (np.shape(y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns (for convenience)\n",
    "test_df = test_ds.to_pandas()\n",
    "test_df[\"text\"] = test_df[\"title\"] + \" \" + test_df[\"description\"]\n",
    "test_df[\"prediction\"] = test_df.index.map(lambda i: preprocessor.index_to_class[y_pred[i]])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarse-grained\n",
    "\n",
    "While we were developing our models, our evaluation process involved computing the coarse-grained metrics such as overall precision, recall and f1 metrics.\n",
    "\n",
    "- True positives (TP): we correctly predicted class X.\n",
    "- False positives (FP): we incorrectly predicted class X but it was another class.\n",
    "- True negatives (TN): we correctly predicted that it's wasn't the class X.\n",
    "- False negatives (FN): we incorrectly predicted that it wasn't the class X but it was.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall metrics\n",
    "overall_metrics = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "metrics[\"overall\"][\"precision\"] = overall_metrics[0]\n",
    "metrics[\"overall\"][\"recall\"] = overall_metrics[1]\n",
    "metrics[\"overall\"][\"f1\"] = overall_metrics[2]\n",
    "metrics[\"overall\"][\"num_samples\"] = np.float64(len(y_test))\n",
    "print (json.dumps(metrics[\"overall\"], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "class_metrics = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "for i, _class in enumerate(preprocessor.class_to_index):\n",
    "    metrics[\"class\"][_class] = {\n",
    "        \"precision\": class_metrics[0][i],\n",
    "        \"recall\": class_metrics[1][i],\n",
    "        \"f1\": class_metrics[2][i],\n",
    "        \"num_samples\": np.float64(class_metrics[3][i]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for a specific class\n",
    "tag=\"natural-language-processing\"\n",
    "print (json.dumps(metrics[\"class\"][tag], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted tags\n",
    "sorted_tags_by_f1 = OrderedDict(sorted(\n",
    "        metrics[\"class\"].items(), key=lambda tag: tag[1][\"f1\"], reverse=True))\n",
    "for item in sorted_tags_by_f1.items():\n",
    "    print (json.dumps(item, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP, FP, FN samples\n",
    "tag = \"natural-language-processing\"\n",
    "index = preprocessor.class_to_index[tag]\n",
    "tp, fp, fn = [], [], []\n",
    "for i, true in enumerate(y_test):\n",
    "    pred = y_pred[i]\n",
    "    if index==true==pred:\n",
    "        tp.append(i)\n",
    "    elif index!=true and index==pred:\n",
    "        fp.append(i)\n",
    "    elif index==true and index!=pred:\n",
    "        fn.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tp)\n",
    "print (fp)\n",
    "print (fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples\n",
    "num_samples = 3\n",
    "cm = [(tp, \"True positives\"), (fp, \"False positives\"), (fn, \"False negatives\")]\n",
    "for item in cm:\n",
    "    if len(item[0]):\n",
    "        print (f\"\\n=== {item[1]} ===\")\n",
    "        for index in item[0][:num_samples]:\n",
    "            print (f\"{test_df.iloc[index].text}\")\n",
    "            print (f\"    true: {test_df.tag[index]}\")\n",
    "            print (f\"    pred: {test_df.prediction[index]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag to inspect\n",
    "tag = \"natural-language-processing\"\n",
    "index = class_to_index[tag]\n",
    "indices = np.where(y_test==index)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence score for the correct class is below a threshold\n",
    "low_confidence = []\n",
    "min_threshold = 0.5\n",
    "for i in indices:\n",
    "    prob = y_prob[i][index]\n",
    "    if prob <= 0.5:\n",
    "        low_confidence.append({\n",
    "            \"text\": f\"{test_df.iloc[i].text}\",\n",
    "            \"true\": test_df.tag[i],\n",
    "            \"pred\": test_df.prediction[i],\n",
    "            \"prob\": prob})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "from cleanlab.filter import find_label_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find label issues\n",
    "label_issues = find_label_issues(labels=y_test, pred_probs=y_prob, return_indices_ranked_by=\"self_confidence\")\n",
    "test_df.iloc[label_issues].drop(columns=[\"text\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.slicing import PandasSFApplier\n",
    "from snorkel.slicing import slice_dataframe\n",
    "from snorkel.slicing import slicing_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def nlp_llm(x):\n",
    "    \"\"\"NLP projects that use LLMs.\"\"\"\n",
    "    nlp_project = \"natural-language-processing\" in x.tag\n",
    "    llm_terms = [\"transformer\", \"llm\", \"bert\"]\n",
    "    llm_project = any(s.lower() in x.text.lower() for s in llm_terms)\n",
    "    return (nlp_project and llm_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def short_text(x):\n",
    "    \"\"\"Projects with short titles and descriptions.\"\"\"\n",
    "    return len(x.text.split()) < 8  # less than 8 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_llm_df = slice_dataframe(test_df, nlp_llm)\n",
    "nlp_llm_df[[\"text\", \"tag\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text_df = slice_dataframe(test_df, short_text)\n",
    "short_text_df[[\"text\", \"tag\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slices\n",
    "slicing_functions = [nlp_llm, short_text]\n",
    "applier = PandasSFApplier(slicing_functions)\n",
    "slices = applier.apply(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score slices\n",
    "metrics[\"slices\"] = {}\n",
    "for slice_name in slices.dtype.names:\n",
    "    mask = slices[slice_name].astype(bool)\n",
    "    if sum(mask):\n",
    "        slice_metrics = precision_recall_fscore_support(\n",
    "            y_test[mask], y_pred[mask], average=\"micro\"\n",
    "        )\n",
    "        metrics[\"slices\"][slice_name] = {}\n",
    "        metrics[\"slices\"][slice_name][\"precision\"] = slice_metrics[0]\n",
    "        metrics[\"slices\"][slice_name][\"recall\"] = slice_metrics[1]\n",
    "        metrics[\"slices\"][slice_name][\"f1\"] = slice_metrics[2]\n",
    "        metrics[\"slices\"][slice_name][\"num_samples\"] = len(y_test[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(metrics[\"slices\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_fn(texts):\n",
    "    df = pd.DataFrame({\"title\": texts, \"description\": \"\", \"tag\": \"other\"})\n",
    "    z = predictor.predict(data=df)[\"predictions\"]\n",
    "    y_prob = torch.tensor(np.stack(z)).softmax(dim=1).numpy()\n",
    "    return y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain instance\n",
    "text = \"Using pretrained convolutional neural networks for object detection.\"\n",
    "explainer = LimeTextExplainer(class_names=list(class_to_index.keys()))\n",
    "explainer.explain_instance(text, classifier_fn=classifier_fn, top_labels=1).show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVariance via verb injection (changes should not affect outputs)\n",
    "tokens = [\"revolutionized\", \"disrupted\"]\n",
    "texts = [f\"Transformers applied to NLP have {token} the ML field.\" for token in tokens]\n",
    "[preprocessor.index_to_class[y_prob.argmax()] for y_prob in classifier_fn(texts=texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRectional expectations (changes with known outputs)\n",
    "tokens = [\"text classification\", \"image classification\"]\n",
    "texts = [f\"ML applied to {token}.\" for token in tokens]\n",
    "[preprocessor.index_to_class[y_prob.argmax()] for y_prob in classifier_fn(texts=texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum Functionality Tests (simple input/output pairs)\n",
    "tokens = [\"natural language processing\", \"mlops\"]\n",
    "texts = [f\"{token} is the next big wave in machine learning.\" for token in tokens]\n",
    "[preprocessor.index_to_class[y_prob.argmax()] for y_prob in classifier_fn(texts=texts)]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
